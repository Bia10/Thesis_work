---
title: How would an historian do time-series econometrics? Analogy weighting using
  machine learning
author: "Jim Savage"
date: "7 July 2015"
output: pdf_document
fontsize: 12pt
linestretch: 1.5
---

# Abstract


# Introduction

In building empirical macroeconomic models for policy simulation and forecasting *today*, how much history should be included? Too much history and the analyst risks estimating parameters that describe history well but have little relevance to today. With too little history, estimates risk missing out on useful information. Implicit in doing time-series work is an assumption that history provides a useful guide for parameterising economic models. 

The method presented in this paper takes this one step further: I assert that if history provides a useful guide, then more analogous histories should provide a better guide. Conversely, we should have little confidence in model projections being made if the current state of the world is markedly different from the world from which the model parameters were estimated. 

How should we decide which histories are most relevant to today? The techniques most commonly used to address this problem aim to account for unmodelled structural change in the underlying data generating process. They come in varying degrees of complexity. 

A naive or un-principled approach is to assume that co-movements between variables over the recent history are likely to emerge from a structural relationship that is relevant for the near future. Making such an assumption motivates techniques often used in industry and government work[cite: BASEL III], such as estimating models over a rolling window, or using exponential smoothing to give less weight to more distant historial observations. These techniques are easy to understand and implement, but they afford many degrees of freedom to the researcher. 

A second class of strategies includes all historical data, but explicitly models the time variation in model parameters, either using regime-switching frameworks or continuous-state approaches. These approaches are more principled, in that the parameters and data come from a generative model (requiring that the researcher be explicit in their beliefs). Yet they are lacking on other dimensions. The models are typically difficult to implement, are richly parameterised (necessitating informative priors), and, often, the generative models themselves give non-zero probability to implausible outcomes, such as explosive states.

How would historians build econometric models? The 'historianist' approach proposed in this paper is to analogise today with periods of history that have similar characteristics, and draw inference based on those analogies. There is a large problem in applying this historianist approach formally to time-series models: how can we determine which time-periods are "similar" to today? Estimating similarity typically involves selecting a metric with which we can measure the difference between periods. But how much weight should be given to the various dimensions? And should these weights change over time? Perhaps one variable is very important in some states of the world and not in others.

This paper makes two contributions. First, it reviews recent literature and presents a model that provides guidance on what variables are likely to be useful in determining "relevant" histories. Second, it describes an algorithm that makes use of a popular method from machine learning to construct "analogy weights". These weights can be used to help regression models learn more from similar periods of history. The weights have two important features. First, their construction does not make use of a metric; a variable's importance to the weight can vary with states of the world. Second, there are automated out-of-sample robustness checks in the construction of the weights, and so weights represent only similarities on dimensions that appear to have a robust relationship to the dependent variable. When used with Bayesian regression models  with informative priors on (large) residual variance parameters, this algorithm can help increase uncertainty when the model is being asked to generate projections in a world that is not similar to the world used to estimate the model. The algorithm also requires very few tuning parameters, making it a useful turn-key solution to many time-series modelling tasks. 

The paper also presents two applations of the algorithm. The first is in estimating "core" inflation---the permanent component of inflation, using a Vector Autoregression (VAR) with analogy weights. The second is an application to portfolio risk management, in particular estimating Value at Risk (VaR) for a small portfolio of stocks during the Global Financial Crisis. This second application illustrates how researchers can easily incorporate analogy weights into multivatiate GARCH models. The applications show that using analogy scores generates the greatest benefit in the first few periods following an unprecedented regime change. 

# 1: Historical analogies as guides

The premise of analogy weighting is that periods of history that are similar to today may provide more information about the structure of contemporaneous economic relationships than less similar periods of history. Why should this be the case? The channel I propose is that economic agents' optimal decisions are conditional on a set of constraints, and the full set of constraints needn't bind all the time. Histories that are similar in terms of which constraints are "most binding" will be those whose agents face similar trade-offs. Under an assumption of static preferences, agents' behaviour will be more similar during analogous periods than during irrelevant periods (during which agents face entirely different constraints). 

Is this channel plausible? Kiyotaki and Moore (1997, 2000) note that in credit markets, a fall in asset prices can make collateral constraints bind, reducing asset purchases and so aplifying credit cycles. In Bianchi (2010), borrowers of foreign currency face similar collateral constraints when their home currency depreciates against the creditor's, reducing the availability of foreign credit and thus making exchange rate depreciations contractionary. The model presented below suggests that the density of projects' internal rates of return is not uniform (and may have little mass at the current discount rate), so that changes in the discount rate can produce non-linear responses investment decisions. These models all suggest that any two histories will be analogous for potentially many reasons, which complicates the estimation of analogy scores. 

Another way of stating the hypothesis is that the parameters of the data generating process that describes the economy are determined by the values of variables constraining agents at a given time. Two economies with precicsely the same values for all economic time series (and their lags) will, in expectation, have the same data generating process. If this hypothesis is correct, then an analogy score that measures the distance between histories in their characteristic space will provide value for estimating the DGP.

In practice, the distance between histories according to their constraining variables (here called the characteristic space) requires the specification of a distance measure---normally a fixed metric. One approach would be to normalise all characteristics and take the Euclidian distance between all pairwise observations. But this would assume that each characteristic is equally meaningful in determining the relevant analogy---a problem if different variables act as "more binding" constraints through history. More complex metrics could allow characteristics to be weighted, yet for as long as the distance is based on a fixed metric, characteristic importance weights would not vary across states of the world. 

The algorithm proposed makes use of the Random Forest (due to Brieman 2001), a tool from machine learning. A biproduct of fitting a Random Forest is a proximity matrix, which for all pairwise observations provides a measure of *proximity* in the Random Forest space. The main benefit of this tool is that the distance measure is the distance between observations *in terms of the variables that matter to the prediction of the outcome variable, and the importance of each variable can vary over time*. 


## A model of discount rate thresholds

Interest rates are an important constraint on economic activity. But do they always bight? The model below suggests that two variables---the quality of product designs and the extent of firms' markets---affect the distribution of "choke-point" discount rates (rates above which projects do not go ahead). Consequently there may be ranges of possible interest rates for which interest rates have almost no effect on investment decisions, and other ranges that profoundly impact investment. 

An economy is made up of $i\in N$ firms generating a unique product $X_{i}$ using capital $K_{i}$ and a design $A_{i}$. Although there is a continuum of designs across product types, there is only one design available for each product, and so firm $i$ uses a fixed $A_{i}$. All firms are local monopolies, and face an inverse demand curve $P(X_{i}|S_{i})$, where $S_{i}$ is the size of their market. The demand curve is downward sloping and smaller markets have a steeper demand curve. The design used in production is non-rival, and so firms have increasing returns to scale. Both designs and product-specific market size are distributed randomly across firms. Designs are generated in a foreign competitive market, and so the cost of design $i$, $C_{i}$ is the marginal cost and entirely exogenous. Capital has some rental rate $r$.

The production technology for firm $i$ is $X_{i} = A_{i}^{\phi}K_{i}^{\alpha}$ with $\phi\geq 1$ and $0<\alpha<1$. The firm maximises profits (denominated in terms of an outside good)

\[
\Pi_{i} = A_{i}^{\phi}K_{i}^{\alpha} - C_{A}A_{i} - rK_{i}
\]

Taking derivatives with respect to capital yields the first order condition

\[
\alpha A_{i}^{\phi}K_{i}^{\alpha-1}(P(X_{i}|S_{i}) + A_{i}^{\phi}K_{i}^{\alpha}P^{'}(X_{i}|S_{i})) = r
\]

Where the term in the brackets is the marginal revenue for a given market size, and the term in front of the brackets is the marginal product of capital. The firm simply sets marginal revenues to marginal costs. 

As for each product there is only one design, the marginal product of designs is not defined. That is, the firm can only choose between using the design and not using the design: $A_{i}\in \{0, \bar{A}_{i}\}$. This means that firm profits have two cases. The first is when $A_{i} = 0$, in which case there are no profits. The second case is when $A_{i} = \bar{A}_{i}$. In this case, profits are

\[
\Pi_{i} = \bar{A}_{i}^{\phi}K_{i}^{\alpha} - C_{A}\bar{A}_{i} - MPK_{i}(\bar{A}_{i})MR(S_{i})K_{i}
\]

in which case the firm operates so long as 

\[
C_{i} \leq \bar{A}_{i}^{\phi-1}K_{i}^{\alpha} - \frac{MPK_{i}(\bar{A}_{i})MR(S_{i})K_{i}}{\bar{A}_{i}}
\]


## Introducing Random Forests

The algorithm described below uses the proximity matrix of a random forest. This chapter describes how the Random Forest works, starting by explaining the Classification and Regression Tree method, and then describes how the proximity matrix is estimated. Readers with a familiarity with these methods may skip to the next chapter. 

## CART

Classification and Regression Trees, known as CART and due to Brieman et al (1984), is a non-linear supervised recursive partitioning method. The aim of the technique is to partition a dataset according to the values of its covariates such that in each of the sub-groups the values of the dependent variable are more similar than in the parent group. The routine described below is a simplified version of the CART routine implemented in many software packages, though one that is much closer to the trees that make up a Random Forest. 

Formally, let $x_{i,p}$ be the $p\in P$-th covariate for individual $i\in I$. Although in some implementations of CART $x$ can be a categorical variable, we consider only the case with a numerical $x$. The collection of all $x$s is $X$. Each individual has a dependent variable $y_{i}\in Y$, which again we take to be numerical.

Each node of the tree---a juncture between branches---is associated with a "split-point" $\bar{x_{p}}$ defined as the point that splits the parent group into two child groups so that the sum of sum of squares for the child groups is reduced by as much as possible:

\[
\bar{x_{p}} = \mbox{argmax}_{x_{p}} \left\{\sum_{i\in I} (y_{i} - \bar{y})^{2} - \left(\sum_{i\in I_{L}} (y_{i} - \bar{y_{L}})^{2} + \sum_{i\in I_{R}} (y_{i} - \bar{y_{R}})^{2}\right)\right\}
\]

Where $I_{L}$ is the "left" part of the data; the individuals $i$ for whom $x_{i,p}<\bar{x_{p}}$. The mean of those individuals' dependent variable is $\bar{y_{L}}$. $I_{R}$ is the remainder of the observations. These split points are simply values of the covariates that appear to be useful in terms of splitting the group into more similar sub-groups. 

CART is a recursive algorithm, in that once a split has been estimated, the two sub-groups are then split, recursively, until further splits result in child groups groups that are no more similar than their parents, or until a minimum node size has been reached. In "pruned" tree, there is some cost attached to making further splits; if the gain in terms of decreased sum of squares does not meet some cost, then the split is not made. This is an efficient way of decreasing over-fitting. 

Once a tree is estimated, it can produce predicted values for a new observation *even if that observation is missing elements of* $X$. A new observation is simply "dropped" down the tree until it either reaches a terminal node (one with no child nodes) or until it reaches a node whose split is defined on a variable that is missing for the individual. Once it reaches this node, its predicted value for $y$ is simply the average $y$ for observations in that node in the training set. 

## Random Forests

While CART models are extremely easy to understand and visualise, they do tend to result in overfitting. They also do not capture smooth relationships well (there will only be as many distinct fitted values as there are terminal nodes). 

Random Forests were designed to address these problems. They are essentially a collection of (normally thousands of) CART models, with two small twists. First, in a Random Forest, each tree is estimated on a random subset of the observations $I$. Second, at each node, only a proportion of all candidate $x_{p}$ are eligible to be split on. All trees are grown to their full extent---that is, not pruned. This results in the trees that make up a Random Forest being quite distinct, there being no individual observation or $x$ variable that has an enormous influence over all trees as a group. 

To generate predicted values from a Random Forest, a new observation $x_{i}$ is passed down all trees, and the fitted value is the average of the fitted values in all trees. 

# Random Forest proximity matrices

This paper is not concerned with the (impressive, see Caruana and Niculescu-Mizil, 2006) predictive abilities of the Random Forest. Rather, the Random Forest generates a "proximity matrix" that gives a distance between all observations in the dataset. 

When two observations fall into the same terminal node as one another, they are said to be *proximate*. The ($i$,$j$)th entry in a Random Forest's proximity matrix is the proportion of terminal nodes that observations $i$ and $j$ share across all trees. For additional robustness, it is desirable to use only "out-of-bag" observations in the calculation of proximities. These are the observations excluded when each tree is grown. In this formulation of the proximity matrix, the proximity score is the proporton of terminal nodes shared by two observations *for the trees that the observations were out-of-bag*.

The proximity matrix is an attractive candidate for a distance measure between historical observations. It does not require defining a distance metric or estimating importance weights for the covariates. Because the Random Forest is supervised (it has a dependent variable), the proximities can be thought of as the distance between observations *in terms of the variables that matter for the prediction of y*. And because of the robust manner in which a Random Forest is estimated, (stationary) independent variables whose relationship with $y$ is coincidental are less likely to influence the proximity score. Consequently, the analyst can afford to include many independent variables; those without a robust relationship will tend to be excluded. 

# Analogy score weighting---the algorithm

The aim of the following algorithm is to generate a vector corresponding to periods $t_{1},\dots,T$ that contains weights denoting the similarity between all historical periods and period $T$. Period $T$ will typically be chosen to be the most recent period, as the model builder will generally want to build a useful model of the current state of the economy. 

The first step is to construct a $T\times P$ matrix $X$ that contains $P$ variables that could be though of as describing the "state" of the economy/market conditions. These variables should be stationary.

An aggregated proximity matrix is then constructed like so:

```
initialise pmat as a T by T matrix of 0s

for p in 1 to P:
  build Random Forest model of X[,p] | X[,-p], call this RF
  pmat = pmat + proximity matrix of RF
end
pmat = 1/P * pmat
```

The $T$-th row of this matrix contains the proximities between period $T$ and all periods beforehand. This vector can be then used for weighting observations in a regression model. 

## Tuning parameters

Given the introduction of randomness in estimating the random forests that we use to estimate analogy weights, how sure should we be that a given proximity is meaningful? That is, is there a sense of "significance" of the proximity estimate? 

Two of the random forest's tuning parameters have the greatest influence on proximity estimates. The first is `ntree`, the number of trees in the forest. A smaller number of trees relative to the number of observations is more likely to generate spurious matches. As the number of trees grows, the stability of proximity weights improves. This can be seen by taking the absolute value of the derivative of average proximity scores with respect to `ntree`. That is:

\[
\mbox{instability score} = \mbox{abs}\left(\frac{\partial \frac{1}{n^{2}}\sum_{i}\sum_{j}\mbox{proximity}_{i,j}}{\partial \mbox{ntree}}\right)
\]

This is illustrated below. As is clear, there are diminishing returns to the number of trees.  

The second main tuning parameter that users should be aware of is the size of terminal nodes in each tree in the random forest, `nodesize`. The default is that trees should grow until a split would not decrease the loss, or until a split would result in a terminal node containing fewer observations than the minimum. The default node size value in `R`'s implementation, `randomForest`, is 5. Increasing this size results in greater average proximity between observations, as the probability that any two observations share a terminal node is greater.

With a single terminal node in each tree, all observations will be trivially proximate, and analogy weights will be 1. This of course would defeat the purpose of using analogy weights! If trees are grown to a full extent (the minimum terminal node size), then a lower bound on the average weight will be the $\frac{5}{\mbox{number of obs}}$. In practice, the average node size will be considerably larger than the minimum node size. This is because candidate splits must both decrease terminal-node variance *and* result in two terminal nodes that exceed the minimum terminal node size.

Choosing the optimal terminal node size will involve making a trade-off between including potentially irrelevant (or misleading) histories for larger values of `modesize`, and having more uncertainty in estimates for smaller values of `nodesize`. The choice should be influenced by the number of parameters in the economic model being estimated, so that on average there are a sufficient number of effective observations (ie. the average weight multiplied by the total number of observations) to provide guidance for parameter values. 

```{r, echo = F, message = F, warning = F}

library(randomForest); library(dplyr); library(ggplot2); library(reshape2)
load("quarterly_data.RData")

dss <- dss %>%
  mutate(Unemp.1 = lag(Unemp),
         Unemp.2 = lag(Unemp, 2),
         CPI.1 = lag(CPI),
         CPI.2 = lag(CPI, 2),
         r.1 = lag(r),
         r.2 = lag(r, 2)) %>% filter(!is.na(CPI.2))
indvar <- dss %>% select(Unemp.1, Unemp.2, r.1, r.2, CPI.1, CPI.2) %>% as.data.frame %>% as.matrix

rf1 <- randomForest(r ~ Unemp.1 + Unemp.2 + CPI.1 + CPI.2 + r.1 + r.2, data = dss, proximity = T, ntree = 5000, nodesize = 40, oob.prox = T)
prox <- rf1$proximity[nrow(rf1$proximity),]

densities <- dss %>% mutate(prox = prox/sum(prox)) %>%
  select(-Unemp, -r, -CPI) %>% melt(id = "prox") %>% 
  ggplot(aes(x = value)) +
  geom_density(aes(weight = 1/length(prox))) +
  geom_density(aes(weight = prox/sum(prox)), colour  = "red") +
  facet_grid(variable~., scales = "free") +
  theme_bw(base_size = 11) +
  ggtitle("KDE of independent variables, weighted and unweighted\n(red = analogy weighted; black = unweighted)")

png("covariate_density.png")
densities
dev.off()
mod.weighted <- lm(dss$r ~ ., data = indvar %>% as.data.frame, weights = prox)
mod.unweighted <- lm(dss$r ~ ., data = indvar %>% as.data.frame, subset = 1:nrow(dss)>(nrow(dss)-round(sum(prox))))
df1 <- data.frame(Coef = names(coef(mod.weighted)), Coef_unweighted = coef(mod.unweighted), Coef_weighted = coef(mod.weighted)) %>% melt(id = "Coef") %>% as.data.frame

df2 <- data.frame(Coef = names(coef(mod.weighted)), Coef_unweighted = arm::se.coef(mod.unweighted), Coef_weighted = arm::se.coef(mod.weighted)) %>% melt(id = "Coef")

df1$se = df2$value
df1 <- df1 %>% mutate(Lower = value - 1.96*se,
         Upper = value + 1.96*se)

coefplot <- df1 %>% melt(id = 1:4, variable.name = "Bounds", value.name = "B") %>% 
  ggplot(aes(x = Coef)) +
  geom_bar(aes(y = value), position = "dodge", stat = "identity", fill = "orange", alpha = 0.5) +
  geom_line(aes(y = B, group = Coef), position = "dodge") +
  facet_grid(.~variable) +
  coord_flip() +
  theme_bw(base_size = 11)

```

This calculation is not straightforward; using analogy weighting can both increase and decrease the precision of parameter estimates. Because analogy scores discount dissimilar observations, they necessarily compress the support on which the parameters are estimated (see figures XX and XX2 in the next chapter). Taken alone, this should increase uncertainty in the parameter estimates. But the entire notion of proximity weighting is that irrelevant histories are just that---and co-movements during irrelevant periods simply confound our understanding of the true relationship today. If the co-movements between dependent and independent variables during analogous histories are less variable, then the precision of parameter estimates will *increase* under analogy weighting. 

### An example of estimating proximity weights

The application below illustrates how one could use analogy weights in estimating the parameters of a simple economic model, and discusses the consequences of modelling choices. 

The procedure of building analogy weights discussed above was carried out on quarterly US data, specifically, the unemployment rate, the rate on short-term US Treasuries, and the quarter-on-quarter percentage change of the major cities consumer price index, taking $T$ to be the first quarter of 2015. Random forests were trained on each of these variables, taking the first and second lags of the others to be predictors, and the out-of-bag proximity matrices were saved. The terminal node size used was 40, about a fifth of the sample.

The plot below shows the resulting proximity matrix. Along the vertical axis is the date for which we would like historical analogies. Along the horizontal axis is the period being evaluated for similarity. The shading indicates the degree of similarity. 

![A proximity matrix](analogy_matrix.pdf)




Interestingly, there appear to be three or four distinct states; the first being the post-GFC period in the top right. The second is a period of relative tranquility, roughly coinciding with the period between the early 1990s recession and the GFC, and the period before the stagflation of the 1970s. The third period, first making itself apparent in the late 1960s and continuing through until the early 1990s, is a less defined period. The emergence of fairly discrete states is interesting, given that the model is one of continuous states. 

Another important feature of the proximity matrix is its ability to inform the user when the current state of the world is without precedent in terms of the variables that help to generate predictions. When the state of the world differs substantially to the sample, then perhaps the implications of models estimated from (a very different) history should be trusted less. 

A metric to evaluate the similarity of the current state to recorded history is to divide the effective sample size (the sum of weights) by the naive sample size (the number of observations) up to a given period. This gives the effective proportion of history up to a period that would be included if you were to use the analogy weights. It is a crude measure for similarity to all of recorded history, illustrated in the figure below. 

![A proximity matrix](relevant_histories.pdf)

As a small illustration of how analogy weights impact parameter estimates and confidence bounds can be seen

```{r, message=F, echo = F}
dss$prox <- prox

uw <- dss %>% ggplot(aes(x = CPI.1, y = r)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw(base_size = 11) +
  xlab("First lag of CPI") +
  ylab("Interest rate") +
  ggtitle("Interest-CPI correlation, no weighting")

library(lubridate)
dss <- dss %>% mutate(Decade = seq(from = as.Date("1960-07-01"), by = "quarter", length.out = nrow(dss)) %>% decimal_date %>% signif(digits = 3))
w <- dss %>% ggplot(aes(x = CPI.1, y = r, weight = prox, size = prox, colour = as.factor(Decade))) +
  geom_point() +
  geom_smooth(method = "lm", colour = "black") +
  theme_bw(base_size = 11) +
  xlab("First lag of CPI") +
  ylab("Interest rate") +
  scale_size(guide = F)+
  ggtitle("Interest-CPI correlation, with weighting")


library(gridExtra)
grid.arrange(uw, w)
```



```{r, echo = F}
library(ggplot2)
coefplot + coord_flip()
```

# 3: An application to estimating core inflation

Most central banks have a mandate to fight inflation, either explicitly, as with a numerical inflation target, or implicitly, as with the Fed's "just do it" approach (Mishkin, 2002). The inflation that is typically targetted is a notion of "core inflation", the inflation that would remain after self-correcting blips have been ignored. This is a latent quantity, and must be estimated. Various methods of estimating core inflation exist, such as removing volatile component price series, or by decomposing price series into factors using principle component analysis. 

## Definition of core inflation

## Three approaches formally

## Data used

## Forecasting performance of the three approaches

## Impulse response functions of the three approaches

# 4: An application to risk management

To come

# 5: Conclustion