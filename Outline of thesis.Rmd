---
title: How would an historian do time-series econometrics? Analogy weighting using
  machine learning
author: "Jim Savage"
date: "7 July 2015"
output: pdf_document
fontsize: 12pt
linestretch: 1.5
---

# Abstract


# Introduction

In building reduced-form empirical macroeconomic models for policy simulation and forecasting *today*, how much history should be included? Too much history and the analyst risks estimating parameters that describe history well but have little relevance to today. With too little history, estimates risk missing out on useful information. Implicit in doing time-series work is an assumption that history provides a useful guide for parameterising economic models. 

The method presented in this paper takes this one step further: I assert that if history provides a useful guide to reduced-form models, then more analogous histories should provide a better guide. Conversely, we should have little confidence in model projections being made if the current state of the world is markedly different from the world from which the model parameters were estimated. 

How should we decide which histories are most relevant to today? The techniques most commonly used to address this problem aim to account for unmodelled structural change in the underlying data generating process. They come in varying degrees of complexity. 

A naive or un-principled approach is to assume that co-movements between variables over the recent history are likely to emerge from a structural relationship that is relevant for the near future. Making such an assumption motivates techniques often used in industry and government work[cite: BASEL III], such as estimating models over a rolling window, or using exponential smoothing to give less weight to more distant historial observations. These techniques are easy to understand and implement, but it is not clear what an optimal window selection rule should be. 

A second class of strategies includes all historical data, but explicitly models the time variation in model parameters, either using regime-switching frameworks or continuous-state approaches. These approaches are more principled, in that the parameters and data come from a generative model (requiring that the researcher be explicit in their beliefs). Yet they are lacking on other dimensions. The models are typically difficult to implement, are richly parameterised (necessitating informative priors), and, often, the generative models themselves give non-zero probability to implausible outcomes, such as explosive states.

How would historians build econometric models? The 'historianist' approach proposed in this paper is to analogise today with periods of history that have similar characteristics, and draw inference based on those analogies. There is a large problem in applying this historianist approach formally to time-series models: how can we determine which time-periods are "similar" to today? Estimating similarity typically involves selecting a metric with which we can measure the difference between periods. But how much weight should be given to the various dimensions? And should these weights change over time? Perhaps one variable is very important in some states of the world and not in others.

This paper describes an algorithm that makes use of the Random Forest (Brieman, 2001) a popular method from machine learning to construct "analogy weights". These weights can be used to help regression models learn more from similar periods of history. In the applications below, analogy weighted models outperform benchmark models in out-of-sample predictive comparisons. Yet the real advantage of using analogy weights is to inform the user when the world is in an unprecedented state. When used with Bayesian regression models with informative priors on (large) residual variance parameters, this algorithm can help increase uncertainty when the model is being asked to generate projections in a world that is not similar to the world used to estimate the model.

The weights have two important features. First, their construction does not make use of a metric; a variable's importance to the weight can vary with states of the world. Second, there are automated out-of-sample robustness checks in the construction of the weights, and so weights represent only similarities on dimensions that appear to have a robust relationship to the dependent variable. The algorithm also requires very few tuning parameters, making it a useful turn-key solution to many time-series modelling tasks. 

The paper also presents two applations of the algorithm. The first is in estimating "core" inflation---the permanent component of inflation, using a Vector Autoregression (VAR) with analogy weights. The second is an application to portfolio risk management, in particular estimating Value at Risk (VaR) for a small portfolio of stocks during the Global Financial Crisis. This second application illustrates how researchers can easily incorporate analogy weights into multivatiate GARCH models. The applications show that using analogy scores generates the greatest benefit in the first few periods following an unprecedented regime change. 

# 1: Historical analogies as guides

The premise of analogy weighting is that periods of history that are similar to today may provide more information about the structure of contemporaneous economic relationships than less similar periods of history. Why may this be the case? This is a largely theoretical question left for further research. Yet the literature contains several appealing models that give some support to the notion that linear historical relationships may break down. These models typically involve constraints that only occasionally bind; on binding, the economy alters its state and behaves differently. Taking these models at face value would imply a limited number of states; these may be efficiently captured using Markov-switching models, so long as all states have been observed. If many such sometimes-binding constraints exist, and are defined on many variables, then continuous-state models may capture 

Is this channel plausible? Kiyotaki and Moore (1997, 2000) note that in credit markets, a fall in asset prices can make collateral constraints bind, reducing asset purchases and so aplifying credit cycles. In Bianchi (2010), borrowers of foreign currency face similar collateral constraints when their home currency depreciates against the creditor's, reducing the availability of foreign credit and thus making exchange rate depreciations contractionary.

Another way of stating the hypothesis is that the parameters of the data generating process that describes the economy are determined by the values of variables constraining agents at a given time. Two economies with precicsely the same values for all economic time series (and their lags) will, in expectation, have the same data generating process. If this hypothesis is correct, then an analogy score that measures the distance between histories in their characteristic space will provide value for estimating the DGP.

In practice, the distance between histories according to their constraining variables (here called the characteristic space) requires the specification of a distance measure---normally a fixed metric. One approach would be to normalise all characteristics and take the Euclidian distance between all pairwise observations. But this would assume that each characteristic is equally meaningful in determining the relevant analogy---a problem if different variables act as "more binding" constraints through history. More complex metrics could allow characteristics to be weighted, yet for as long as the distance is based on a fixed metric, characteristic importance weights would not vary across states of the world. 

The algorithm proposed makes use of the Random Forest (due to Brieman 2001), a tool from machine learning. A biproduct of fitting a Random Forest is a proximity matrix, which for all pairwise observations provides a measure of *proximity* in the Random Forest space. The main benefit of this tool is that the distance measure is the distance between observations *in terms of the variables that matter to the prediction of the outcome variable, and the importance of each variable can vary over time*. The algorithm is a time-series generalisation of the approach taken by Miller Savage and Tan (forthcoming), in which Random Forest proximities are used to construct a matching estimator that is less sensitive to the Smith and Todd critique than is Propensity Score Matching. 

## Introducing Random Forests

The algorithm described below uses the proximity matrix of a random forest. This chapter describes how the Random Forest works, starting by explaining the Classification and Regression Tree method, and then describes how the proximity matrix is estimated. Readers with a familiarity with these methods may skip to the next chapter. 

## CART

Classification and Regression Trees, known as CART and due to Brieman et al (1984), is a non-linear supervised recursive partitioning method. The aim of the technique is to partition a dataset according to the values of its covariates such that in each of the sub-groups the values of the dependent variable are more similar than in the parent group. The routine described below is a simplified version of the CART routine implemented in many software packages, though one that is much closer to the trees that make up a Random Forest. 

Formally, let $x_{i,p}$ be the $p\in P$-th covariate for individual $i\in I$. Although in some implementations of CART $x$ can be a categorical variable, we consider only the case with a numerical $x$. The collection of all $x$s is $X$. Each individual has a dependent variable $y_{i}\in Y$, which again we take to be numerical.

Each node of the tree---a juncture between branches---is associated with a "split-point" $\bar{x_{p}}$ defined as the point that splits the parent group into two child groups so that the sum of sum of squares for the child groups is reduced by as much as possible:

\[
\bar{x_{p}} = \mbox{argmax}_{x_{p}} \left\{\sum_{i\in I} (y_{i} - \bar{y})^{2} - \left(\sum_{i\in I_{L}} (y_{i} - \bar{y_{L}})^{2} + \sum_{i\in I_{R}} (y_{i} - \bar{y_{R}})^{2}\right)\right\}
\]

Where $I_{L}$ is the "left" part of the data; the individuals $i$ for whom $x_{i,p}<\bar{x_{p}}$. The mean of those individuals' dependent variable is $\bar{y_{L}}$. $I_{R}$ is the remainder of the observations. These split points are simply values of the covariates that appear to be useful in terms of splitting the group into more similar sub-groups. 

CART is a recursive algorithm, in that once a split has been estimated, the two sub-groups are then split, recursively, until further splits result in child groups groups that are no more similar than their parents, or until a minimum node size has been reached. In "pruned" tree, there is some cost attached to making further splits; if the gain in terms of decreased sum of squares does not meet some cost, then the split is not made. This is an efficient way of decreasing over-fitting. 

Once a tree is estimated, it can produce predicted values for a new observation *even if that observation is missing elements of* $X$. A new observation is simply "dropped" down the tree until it either reaches a terminal node (one with no child nodes) or until it reaches a node whose split is defined on a variable that is missing for the individual. Once it reaches this node, its predicted value for $y$ is simply the average $y$ for observations in that node in the training set.

If two observations reach the same terminal node, they are said to be *proximate*. Two proximate observations are similar in terms of all variables on which splitting decisions are made.

## Random Forests

While CART models are extremely easy to understand and visualise, they do tend to result in overfitting. They also do not capture smooth relationships well (there will only be as many distinct predicted values as there are terminal nodes). 

Random Forests were designed to address these problems. They are essentially a collection of (normally thousands of) CART models, with two small twists. First, in a Random Forest, each tree is estimated on a random subset of the observations $I$. Second, at each node, only a proportion of all candidate $x_{p}$ are eligible to be split on. All trees are grown to their full extent---that is, not pruned. This results in the trees that make up a Random Forest being quite distinct, there being no individual observation or $x$ variable that has an enormous influence over all trees as a group. 

To generate predicted values from a Random Forest, a new observation $x_{i}$ is passed down all trees, and the fitted value is the average of the fitted values in all trees. 

# Random Forest proximity matrices

This paper is not concerned with the (impressive, see Caruana and Niculescu-Mizil, 2006) predictive abilities of the Random Forest. Rather, the Random Forest generates a "proximity matrix" that gives a distance between all observations in the dataset. 

When two observations fall into the same terminal node as one another, they are said to be *proximate*. The ($i$,$j$)th entry in a Random Forest's proximity matrix is the proportion of terminal nodes that observations $i$ and $j$ share across all trees. For additional robustness, it is desirable to use only "out-of-bag" observations in the calculation of proximities. These are the observations excluded when each tree is grown. In this formulation of the proximity matrix, the proximity score is the proporton of terminal nodes shared by two observations *for the trees that the observations were out-of-bag*.

The proximity matrix is an attractive candidate for a distance measure between historical observations. It does not require defining a distance metric or estimating importance weights for the covariates. Because the Random Forest is supervised (it has a dependent variable), the proximities can be thought of as the distance between observations *in terms of the variables that matter for the prediction of y*. And because of the robust manner in which a Random Forest is estimated, (stationary) independent variables whose relationship with $y$ is coincidental are less likely to influence the proximity score. Consequently, the analyst can afford to include many independent variables; those without a robust relationship will tend to be excluded. 

# Analogy score weighting---the algorithm

The aim of the following algorithm is to generate a vector corresponding to periods $t_{1},\dots,T$ that contains weights denoting the similarity between all historical periods and period $T$. Period $T$ will typically be chosen to be the most recent period, as the model builder will generally want to build a useful model of the current state of the economy. 

The first step is to construct a $T\times P$ matrix $X$ that contains $P$ variables that could be though of as describing the "state" of the economy/market conditions. These variables should be stationary.

An aggregated proximity matrix is then constructed like so:

```
initialise pmat as a T by T matrix of 0s

for p in 1 to P:
  build Random Forest model of X[,p] | X[,-p], call this RF
  pmat = pmat + proximity matrix of RF
end
pmat = 1/P * pmat
```

The $T$-th row of this matrix contains the proximities between period $T$ and all periods beforehand. This vector can be then used for weighting observations in a regression model. 

## Tuning parameters

Given the introduction of randomness in estimating the random forests that we use to estimate analogy weights, how sure should we be that a given proximity is meaningful? That is, is there a sense of "significance" of the proximity estimate? 

Two of the random forest's tuning parameters have the greatest influence on proximity estimates. The first is `ntree`, the number of trees in the forest. A smaller number of trees relative to the number of observations is more likely to generate spurious matches. As the number of trees grows, the stability of proximity weights improves. This can be seen by taking the absolute value of the derivative of average proximity scores with respect to `ntree`. That is:

\[
\mbox{instability score} = \mbox{abs}\left(\frac{\partial \frac{1}{n^{2}}\sum_{i}\sum_{j}\mbox{proximity}_{i,j}}{\partial \mbox{ntree}}\right)
\]

This is illustrated below. As is clear, there are diminishing returns to the number of trees.  

The second main tuning parameter that users should be aware of is the size of terminal nodes in each tree in the random forest, `nodesize`. The default is that trees should grow until a split would not decrease the loss, or until a split would result in a terminal node containing fewer observations than the minimum. The default node size value in `R`'s implementation, `randomForest`, is 5. Increasing this size results in greater average proximity between observations, as the probability that any two observations share a terminal node is greater.

With a single terminal node in each tree, all observations will be trivially proximate, and analogy weights will be 1. This of course would defeat the purpose of using analogy weights! If trees are grown to a full extent (the minimum terminal node size), then a lower bound on the average weight will be the $\frac{5}{\mbox{number of obs}}$. In practice, the average node size will be considerably larger than the minimum node size. This is because candidate splits must both decrease terminal-node variance *and* result in two terminal nodes that exceed the minimum terminal node size.

Choosing the optimal terminal node size will involve making a trade-off between including potentially irrelevant (or misleading) histories for larger values of `modesize`, and having more uncertainty in estimates for smaller values of `nodesize`. The choice should be influenced by the number of parameters in the economic model being estimated, so that on average there are a sufficient number of effective observations (ie. the average weight multiplied by the total number of observations) to provide guidance for parameter values. 

```{r, echo = F, message = F, warning = F}

library(randomForest); library(dplyr); library(ggplot2); library(reshape2)
load("quarterly_data.RData")

dss <- dat1 %>% as.data.frame %>%
  mutate(Unemp.1 = lag(Unemp),
         Unemp.2 = lag(Unemp, 2),
         CPI.1 = lag(CPI),
         CPI.2 = lag(CPI, 2),
         r.1 = lag(r),
         r.2 = lag(r, 2)) %>% filter(!is.na(CPI.2))
indvar <- dss %>% select(Unemp.1, Unemp.2, r.1, r.2, CPI.1, CPI.2) %>% as.data.frame %>% as.matrix

rf1 <- randomForest(r ~ Unemp.1 + Unemp.2 + CPI.1 + CPI.2 + r.1 + r.2, data = dss, proximity = T, ntree = 5000, nodesize = 40, oob.prox = T)
prox <- rf1$proximity[nrow(rf1$proximity),]

densities <- dss %>% mutate(prox = prox/sum(prox)) %>%
  select(-Unemp, -r, -CPI) %>% melt(id = "prox") %>% 
  ggplot(aes(x = value)) +
  geom_density(aes(weight = 1/length(prox))) +
  geom_density(aes(weight = prox/sum(prox)), colour  = "red") +
  facet_grid(variable~., scales = "free") +
  theme_bw(base_size = 11) +
  ggtitle("KDE of independent variables, weighted and unweighted\n(red = analogy weighted; black = unweighted)")

png("covariate_density.png")
densities
dev.off()

mod.weighted <- lm(dss$r ~ ., data = indvar %>% as.data.frame, weights = prox)
mod.unweighted <- lm(dss$r ~ ., data = indvar %>% as.data.frame, subset = 1:nrow(dss)>(nrow(dss)-round(sum(prox))))
df1 <- data.frame(Coef = names(coef(mod.weighted)), Coef_unweighted = coef(mod.unweighted), Coef_weighted = coef(mod.weighted)) %>% melt(id = "Coef") %>% as.data.frame

df2 <- data.frame(Coef = names(coef(mod.weighted)), Coef_unweighted = arm::se.coef(mod.unweighted), Coef_weighted = arm::se.coef(mod.weighted)) %>% melt(id = "Coef")

df1$se = df2$value
df1 <- df1 %>% mutate(Lower = value - 1.96*se,
         Upper = value + 1.96*se)

coefplot <- df1 %>% melt(id = 1:4, variable.name = "Bounds", value.name = "B") %>% 
  ggplot(aes(x = Coef)) +
  geom_bar(aes(y = value), position = "dodge", stat = "identity", fill = "orange", alpha = 0.5) +
  geom_line(aes(y = B, group = Coef), position = "dodge") +
  facet_grid(.~variable) +
  coord_flip() +
  theme_bw(base_size = 11)

```

This calculation is not straightforward; using analogy weighting can both increase and decrease the precision of parameter estimates. Because analogy scores discount dissimilar observations, they necessarily compress the support on which the parameters are estimated (see figures XX and XX2 in the next chapter). Taken alone, this should increase uncertainty in the parameter estimates. But the entire notion of proximity weighting is that irrelevant histories are just that---and co-movements during irrelevant periods simply confound our understanding of the true relationship today. If the co-movements between dependent and independent variables during analogous histories are less variable, then the precision of parameter estimates will *increase* under analogy weighting. 

### An example of estimating proximity weights

The application below illustrates how one could use analogy weights in estimating the parameters of a simple economic model, and discusses the consequences of modelling choices. 

The procedure of building analogy weights discussed above was carried out on quarterly US data, specifically, the unemployment rate, the rate on short-term US Treasuries, and the quarter-on-quarter percentage change of the major cities consumer price index, taking $T$ to be the first quarter of 2015. Random forests were trained on each of these variables, taking the first and second lags of the others to be predictors, and the out-of-bag proximity matrices were saved. The terminal node size used was 40, about a fifth of the sample.

The plot below shows the resulting proximity matrix. Along the vertical axis is the date for which we would like historical analogies. Along the horizontal axis is the period being evaluated for similarity. The shading indicates the degree of similarity. 

![A proximity matrix](analogy_matrix.pdf)


Interestingly, there appear to be three or four distinct states; the first being the post-GFC period in the top right. The second is a period of relative tranquility, roughly coinciding with the period between the early 1990s recession and the GFC, and the period before the stagflation of the 1970s. The third period, first making itself apparent in the late 1960s and continuing through until the early 1990s, is a less defined period. The emergence of fairly discrete states is interesting, given that the model is one of continuous states. 

Another important feature of the proximity matrix is its ability to inform the user when the current state of the world is without precedent in terms of the variables that help to generate predictions. When the state of the world differs substantially to the sample, then perhaps the implications of models estimated from (a very different) history should be trusted less. 

A metric to evaluate the similarity of the current state to recorded history is to divide the effective sample size (the sum of weights) by the naive sample size (the number of observations) up to a given period. This gives the effective proportion of history up to a period that would be included if you were to use the analogy weights. It is a crude measure for similarity to all of recorded history, illustrated in the figure below. 

![A proximity matrix](relevant_histories.pdf)

As a small illustration of how analogy weights impact parameter estimates and confidence bounds can be seen

```{r, message=F, echo = F}
dss$prox <- prox

uw <- dss %>% ggplot(aes(x = CPI.1, y = r)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw(base_size = 11) +
  xlab("First lag of CPI") +
  ylab("Interest rate") +
  ggtitle("Interest-CPI correlation, no weighting")

library(lubridate)
dss <- dss %>% mutate(Decade = seq(from = as.Date("1960-07-01"), by = "quarter", length.out = nrow(dss)) %>% decimal_date %>% signif(digits = 3))
w <- dss %>% ggplot(aes(x = CPI.1, y = r, weight = prox, size = prox, colour = as.factor(Decade))) +
  geom_point() +
  geom_smooth(method = "lm", colour = "black") +
  theme_bw(base_size = 11) +
  xlab("First lag of CPI") +
  ylab("Interest rate") +
  scale_size(guide = F)+
  ggtitle("Interest-CPI correlation, with weighting")


library(gridExtra)
#grid.arrange(uw, w)
```



```{r, echo = F}
library(ggplot2)
coefplot + coord_flip()
```

# 3: An application to estimating core inflation

Most central banks have a mandate to fight inflation, either explicitly, as with a numerical inflation target, or implicitly, as with the Fed's "just do it" approach (Mishkin, 2002). The inflation that is typically targetted is a notion of "core inflation", the inflation that would remain after self-correcting blips have been ignored. This is a latent quantity, and must be estimated. Various methods of estimating core inflation exist, such as removing volatile component price series, or by decomposing price series into factors using principle component analysis. 

## Core inflation

Core inflation is a widely-used concept that lacks a formal definition or common tool for estimation, though two conceptual approaches are common.

The first commonly-used approach is to model core inflation as being some latent factor that drives observed price series. For instance, Kapetanias (2002) defines core inflation as the first factor from a singular value decomposition of a large number of component price series. This is similar to the approach of Stock and Watson (2008), who model core inflation using a dynamic factor model that allows for structural breaks. Simpler variants of this modelling philosophy work by filtering or smoothing price series, or by simply taking averages across price series that have had surprising components removed ad-hoc (see Quah and Vahey, 1995).

A second, more principled approach is to model core inflation as being the permanent component of inflation, and therefore the part that has no effect on medium or long-run economic activity. Quah and Vahey (1995), Ribba (2003) and Cogley and Sargent (2001, 2005) take this approach. Cogley and Sargent’s (2001, 2005) method is to take core inflation to be the median of posterior predictive distributions for inflation into the distant future, using a Bayesian VAR with time-varying parameters. Quah and Vahey (1995) define core inflation to be the rate of inflation that has no impact on middle to long-run growth (estimated using a restricted classical VAR).

There are problems with both approaches. The dynamic factor model approach does not typically discriminate between periods in which a signal variable contains a lot of information about the unobserved factor and when it contains less. It is also sensitive to the choice of variables used as inputs to the factors. By dropping or censoring signal series, ad-hoc approaches may not have this problem, however are both costly and atheoretic. In the second approach, an economic model must be built that provides the best possible (long-run) forecasts, given information available at the time the forecast is being made. Thus the problem in estimating core inflation using this approach is one of finding a good forecasting model, and parameterising it with the most relevant parameters for making forecasts. 

In the demonstration below, I use the second approach to estimating core inflation. The demonstration makes use of three models: the first is a vanilla VAR with static parameters. The second is a time-varying parameter VAR based on the Cogley and Sargent's (2001) model. The third model is a vanilla VAR augmented to make use of analogy weights. 

## Three approaches formally

### Vanilla VAR

The model describes the co-movement between inflation, unemployment and the yield on short-term Treasuries, using a VAR(2). Taking $Y_{t}$ to be vector containing all variables, the first model is 

\[
\mathbf{Y_{t} = \mu + \beta_{1}Y_{t-1} + \beta_{2}Y_{t-2} + \epsilon_{t}}
\]

where $\mu$ and the $\beta$s are coefficients and the multivariate residual $\epsilon\sim\mathcal{MVN}(0, \Sigma)$. The parameters are estimated in a Bayesian framework, and have so-called "shrinkage" priors applied. These are all equal to 0 except the AR(1) terms, which have a prior of 1. The residual matrix $\Sigma$ is decomposed into a correlation matrix $\Omega$ and a scale vector $\tau$ such that 

\[
\Sigma = \mbox{diag}(\tau)\Omega\mbox{diag}(\tau)
\]

Under this setup, we give independent priors to the correlation and scale: the scale vectors have a prior distribution of $\tau\sim \mbox{Cauchy}_{+}(0, 2.5)$, and the correlation comes from the the LKJ distribution $\Omega \sim\mbox{lkj}(1.5)$. The LKJ distribution is a quite new distribution for correlation matrices that addresses some of the shortcomings of the popular Inverse-Wishart prior. The single parameter of the distribution dictates how much cross-correlation the distribution will produce; a value of 1 results in a uniform prior, while as this degrees of freedom parameter goes to infinity, the distribution collapses on the unit-diagonal correlation matrix. 

I experimented with two approaches to modelling the covariance between the coefficients $\mu$, $\beta_1$ and $\beta_2$. Estimating a full covariance matrix would be too burdonsome given the amount of data available (it would involve estimating 210 parameters in the covariance matrix alone). One approach is to use a Littermann prior, which restricts $\Omega$ to be diagonal and specifies priors on $\tau$. Another approach is to estimate only the covariances within each "block"; that is, allow $\mu$ to have an estimated covariance, and likewise with the $\beta$s. This has the advantage that it only requires estimating 78 covariance parameters, and was the approach taken for this model. 

The model is estimated using Hamiltonian Monte Carlo HMC, a technique that aims to draw directly from the posterior density using Hamiltonian mechanics. The particular implementation of HMC used is called the No U-Turn Sampler (Hoffman & Gelman, 2011), which is implemented in the stan modelling language. 

While being fitted, the model calculates 1-year-ahead posterior predictions; these make use of both the uncertainty in the regression error and uncertainty in parameter estimates. These predictions form the basis for the model comparisons below. 

### Augmented VAR

The second model fitted takes exactly the same form as the VAR described above, with one difference: it is fitted using analogies weights. These weights are calculated as in Section XX above, using inflation, unemployment and the interest rate as the variables used to estimate analogies. These weights could be thought of as affecting the residual covariance of the VAR process:

\[
\Sigma_{\mbox{Analogy}} = \frac{1}{\mbox{analogy weight}_{t}}\hat{\Sigma}
\]

In this case, very small analogies would be expected to have very large residuals, and consequently not inform the coefficient estimates in any meaningful way. 

### Time-varying parameter VAR

The third model considered is a Time-varying-parameter VAR, similar to the one described in Cogley and Sargent (2001). The model is 

\[
\mathbf{Y_{t} = \mu_{t} + \beta_{1,t}Y_{t-1} + \beta_{2,t}Y_{t-2} + \epsilon_{t}}
\]

with the coefficients themselves evolving over time. Take $\theta = [\mu, \mbox{vec}\beta_1,\mbox{vec}\beta_2]'$, where the $\mbox{vec}$ operation stacks the a matrix in column-major order, then:

\[
\theta_{t} = \theta_{t-1} + \eta_{t}
\]

and $\eta\sim\mathcal{MVN}(0, \Sigma_{\eta})$. For identifiability, suitable structure must be put on $\Sigma_{\eta}$; in the work illustrated below, I assume that it is diagonal. 


## Data used

The data used come from the vintage data releases contained in the ALFRED database. The data series are: The annualised CPI growth rate for major cities; the national non-farm unemployment rate; and the annualised yield on 90 day Treasuries. 

## Forecasting performance of the three approaches

# 5: Conclustion